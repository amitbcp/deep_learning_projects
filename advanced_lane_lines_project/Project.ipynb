{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Lane Finding Project\n",
    "\n",
    "The goals / steps of this project are the following:\n",
    "\n",
    "* Compute the camera calibration matrix and distortion coefficients given a set of chessboard images.\n",
    "* Apply a distortion correction to raw images.\n",
    "* Use color transforms, gradients, etc., to create a thresholded binary image.\n",
    "* Apply a perspective transform to rectify binary image (\"birds-eye view\").\n",
    "* Detect lane pixels and fit to find the lane boundary.\n",
    "* Determine the curvature of the lane and vehicle position with respect to center.\n",
    "* Warp the detected lane boundaries back onto the original image.\n",
    "* Output visual display of the lane boundaries and numerical estimation of lane curvature and vehicle position."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0 : Necessary imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from matplotlib.patches import Polygon\n",
    "import numpy as np\n",
    "\n",
    "from moviepy.editor import VideoFileClip\n",
    "from IPython.display import HTML\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper method to plot two images side by side\n",
    "def plt_images(img_1, title_1, img_2, title_2, cmap='gray'):\n",
    "    # Visualize undirstorsion\n",
    "    f, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))\n",
    "    ax1.set_title(title_1, fontsize=16)\n",
    "    ax1.imshow(img_1)\n",
    "    ax2.set_title(title_2, fontsize=16)\n",
    "    ax2.imshow(img_2, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Compute the camera calibration using chessboard images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calibrate_camera(image_path, nx=9, ny=6):\n",
    "    \"\"\" This method goes over multiple raw images to detect ChessBoard corner points used for calibration\n",
    "\n",
    "        Args:\n",
    "            image_path ([type]): path to the image folder used to calibrate the camera\n",
    "            nx (int): Number of inside corners in x\n",
    "            ny (int): Number of inside corners in y\n",
    "\n",
    "        Returns:\n",
    "            tupple: (objpoints , imgpoints , mtx , dist) from calibration\n",
    "    \"\"\"\n",
    "    \n",
    "    # Make a list of calibration images\n",
    "    images = glob.glob(image_path)\n",
    "\n",
    "    # Prepare object points, like (0,0,0), (1,0,0), (2,0,0) ....,(6,5,0)\n",
    "    objp = np.zeros((nx * ny, 3), np.float32)\n",
    "    objp[:,:2] = np.mgrid[0:nx, 0:ny].T.reshape(-1, 2)\n",
    "\n",
    "    # Arrays to store object points and image points from all the images.\n",
    "    objpoints = [] # 3d points in real world space\n",
    "    imgpoints = [] # 2d points in image plane.\n",
    "\n",
    "    # Step through the list and search for chessboard corners\n",
    "    img = None\n",
    "    for fname in images:    \n",
    "        img = cv2.imread(fname)\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "        # Find the chessboard corners\n",
    "        ret, corners = cv2.findChessboardCorners(gray, (nx, ny), None)\n",
    "\n",
    "        # If found, add object points, image points\n",
    "        if ret == True:\n",
    "            objpoints.append(objp)\n",
    "            imgpoints.append(corners)\n",
    "\n",
    "    # Camera calibration, given object points, image points, and the shape of the grayscale image\n",
    "    if (len(objpoints) > 0):\n",
    "        # Camera successfully calibrated.\n",
    "        print(\"Camera successfully calibrated.\")\n",
    "        ret, mtx, dist, rvecs, tvecs = cv2.calibrateCamera(objpoints, imgpoints, gray.shape[::-1], None, None)\n",
    "    else:\n",
    "        # Unable to calibrate the camera.\n",
    "        print(\"Unable to calibrate the camera.\")\n",
    "        ret, mtx, dist, rvecs, tvecs = (None, None, None, None, None)\n",
    "\n",
    "    return (objpoints , imgpoints , mtx , dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Camera successfully calibrated.\n"
     ]
    }
   ],
   "source": [
    "# Testing the function\n",
    "# Start Camera Calibration\n",
    "calibration = calibrate_camera('./camera_cal/*.*')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Apply a distortion correction to raw images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def undistort(calibration, img, display=True, read = True):\n",
    "    \"\"\"Remove distortion from images\n",
    "\n",
    "        Args:\n",
    "            calibration ([type]): [description]\n",
    "            image ([type]): [description]\n",
    "            display ([type]): [description]\n",
    "            read ([type]): [description]\n",
    "\n",
    "        Returns:\n",
    "            [type]: [description]\n",
    "    \"\"\"\n",
    "    if read:\n",
    "        img = cv2.imread(image)\n",
    "    img_size = (img.shape[1], img.shape[0])\n",
    "    #ret, mtx, dist, rvecs, tvecs = cv2.calibrateCamera(objpoints, imgpoints, img_size, None, None)\n",
    "    undist = cv2.undistort(img, calibration[2], calibration[3], None, calibration[2])\n",
    "    \n",
    "    if display:\n",
    "        f, (ax1, ax2) = plt.subplots(1, 2, figsize=(9,6))\n",
    "        ax1.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "        ax1.set_title('Original Image', fontsize=20)\n",
    "        ax2.imshow(cv2.cvtColor(undist, cv2.COLOR_BGR2RGB))\n",
    "        ax2.set_title('Undistorted Image', fontsize=20)\n",
    "    else:\n",
    "        return undist "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the function\n",
    "# Comment out this block for final run\n",
    "\n",
    "images = glob.glob('test_images/test*.jpg')\n",
    "for image in images:\n",
    "    undistort(calibration,image)\n",
    "    break "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Apply a perspective transform to rectify binary image (\"birds-eye view\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def birds_eye(img, calibration = None, display=True, read = True):\n",
    "    \"\"\"Perform perspective transform\n",
    "\n",
    "        Args:\n",
    "            variable1 ([type]): [description]\n",
    "            variable2 ([type]): [description]\n",
    "\n",
    "        Returns:\n",
    "            [type]: [description]\n",
    "    \"\"\"\n",
    "    \n",
    "    # If calibration is required to undistort the image\n",
    "    if calibration :\n",
    "        if read:\n",
    "            undist = undistort(calibration,img, display = False)\n",
    "        else:\n",
    "            undist = undistort(calibration,img, display = False, read=False) \n",
    "    else :\n",
    "        undist = img\n",
    "    img_size = (undist.shape[1], undist.shape[0])        \n",
    "    \n",
    "    offset = 0\n",
    "#     src_coordinates = np.float32([[490, 482],[810, 482],\n",
    "#                       [1250, 720],[40, 720]])\n",
    "#     dst_coordinates = np.float32([[0, 0], [1280, 0], \n",
    "#                      [1250, 720],[40, 720]])\n",
    "\n",
    "    src_coordinates = np.float32([[581., 477],[699, 477],\n",
    "                      [896, 675],[384, 675]])\n",
    "    dst_coordinates = np.float32([[384, 0], [896, 0], \n",
    "                     [896, 720],[384, 720]])\n",
    "    \n",
    "    M = cv2.getPerspectiveTransform(src_coordinates, dst_coordinates)\n",
    "    warped = cv2.warpPerspective(undist, M, img_size)\n",
    "    Minv = cv2.getPerspectiveTransform(dst_coordinates, src_coordinates)\n",
    "    \n",
    "    if display:\n",
    "        print(\"Un-distorted Image Shape  : {}\".format(undist.shape))\n",
    "        f, (ax1, ax2) = plt.subplots(1, 2, figsize=(9, 6))\n",
    "        f.tight_layout()\n",
    "        ax1.imshow(cv2.cvtColor(undist, cv2.COLOR_BGR2RGB))\n",
    "        ax1.set_title('Undistorted Image', fontsize=20)\n",
    "        ax2.imshow(cv2.cvtColor(warped, cv2.COLOR_BGR2RGB))\n",
    "        ax2.set_title('Undistorted and Warped Image', fontsize=20)\n",
    "        plt.subplots_adjust(left=0., right=1, top=0.9, bottom=0.)\n",
    "    else:\n",
    "        return warped, M , Minv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the function\n",
    "# Comment out this block for final run\n",
    "\n",
    "for image in glob.glob('test_images/test*.jpg'):\n",
    "    birds_eye(image,calibration,display = True)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for image in glob.glob('test_images/test*.jpg'):\n",
    "    warped, M , Minv = birds_eye(image,calibration,display = False)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Use color transforms, gradients, etc., to create a thresholded binary image.\n",
    "\n",
    "We need to apply basic image transformation to detect/highlight the lane in the image/video.\n",
    "We use the following for it -\n",
    "\n",
    "1. Sobel Filters for edge detection\n",
    "2. Gradient Magnitude\n",
    "3. Directional Gradient\n",
    "4. Color Thresholds in different color spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4.1: Sobel Filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function that applies Sobel x or y, \n",
    "#then takes an absolute value and applies a threshold.\n",
    "def abs_sobel_thresh(img, orient='x', sobel_kernel=3, thresh=(20, 100)):\n",
    "    \n",
    "    # Apply the following steps to img\n",
    "    \n",
    "    # 1) Converting to grayscale\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    \n",
    "    # 2) Taking the derivative in x or y given orient = 'x' or 'y'\n",
    "    sobel = cv2.Sobel(gray, cv2.CV_64F, orient=='x', orient=='y')\n",
    "    \n",
    "    # 3) Taking the absolute value of the derivative or gradient\n",
    "    abs_sobel = np.absolute(sobel)\n",
    "    \n",
    "    # 4) Scaling to 8-bit (0 - 255) then convert to type = np.uint8\n",
    "    scaled_sobel = np.uint8(255*abs_sobel/np.max(abs_sobel))\n",
    "    \n",
    "    # 5) Creating a mask of 1's where the scaled gradient magnitude \n",
    "            # is > thresh_min and < thresh_max\n",
    "    sxbinary = np.zeros_like(scaled_sobel)\n",
    "    sxbinary[(scaled_sobel >= thresh[0]) & (scaled_sobel <= thresh[1])] = 1\n",
    "    \n",
    "    # 6) Return this mask as your binary_output image\n",
    "    binary_output = sxbinary # Remove this line\n",
    "    return binary_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4.2: Calculate gradient magnitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that applies Sobel x and y, \n",
    "# then computes the magnitude of the gradient\n",
    "# and applies a threshold\n",
    "\n",
    "def mag_thresh(img, sobel_kernel=3, mag_thresh=(30, 100)) :\n",
    "    \n",
    "    # 1) Convert to grayscale\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    \n",
    "    # 2) Take the gradient in x and y separately\n",
    "    sobelx = cv2.Sobel(gray, cv2.CV_64F, 1, 0)\n",
    "    sobely = cv2.Sobel(gray, cv2.CV_64F, 0, 1)\n",
    "    \n",
    "    # 3) Calculate the magnitude \n",
    "    mag_sobel = np.sqrt(np.square(sobelx) + np.square(sobely))\n",
    "    \n",
    "    # 4) Scale to 8-bit (0 - 255) and convert to type = np.uint8\n",
    "    scaled_sobel = np.uint8(255*mag_sobel/np.max(mag_sobel))\n",
    "    \n",
    "    # 5) Create a binary mask where mag thresholds are met\n",
    "    sxbinary = np.zeros_like(scaled_sobel)\n",
    "    sxbinary[(scaled_sobel >= mag_thresh[0]) & (scaled_sobel <= mag_thresh[1])] = 1\n",
    "    \n",
    "    # 6) Return this mask as your binary_output image\n",
    "    binary_output = np.copy(sxbinary) \n",
    "    \n",
    "    return binary_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4.3: Calculate directional gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that applies Sobel x and y, \n",
    "# then computes the direction of the gradient\n",
    "# and applies a threshold.\n",
    "def dir_thresh(img, sobel_kernel=9, dir_thresh=(0, np.pi/2)):\n",
    "    # Grayscale\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "    # Calculate the x and y gradients\n",
    "    sobelx = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=sobel_kernel)\n",
    "    sobely = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=sobel_kernel)\n",
    "\n",
    "    # Take the absolute value of the gradient direction, \n",
    "    # apply a threshold, and create a binary image result\n",
    "    absgraddir = np.arctan2(np.absolute(sobely), np.absolute(sobelx))\n",
    "    binary_output =  np.zeros_like(absgraddir)\n",
    "    binary_output[(absgraddir >= dir_thresh[0]) & (absgraddir <= dir_thresh[1])] = 1\n",
    "\n",
    "    # Return the binary image\n",
    "    return binary_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4.4: Calculate color threshold\n",
    "\n",
    "In this step I attempted to convert the warped image to different color spaces and create binary thresholded images which highlight only the lane lines and ignore everything else. I found that the following color channels and thresholds did a good job of identifying the lane lines in the provided test images:\n",
    "\n",
    "The S Channel from the HLS color space, with a min threshold of 180 and a max threshold of 255, did a fairly good job of identifying both the white and yellow lane lines, but did not pick up 100% of the pixels in either one, and had a tendency to get distracted by shadows on the road.\n",
    "The L Channel from the LUV color space, with a min threshold of 225 and a max threshold of 255, did an almost perfect job of picking up the white lane lines, but completely ignored the yellow lines.\n",
    "The B channel from the Lab color space, with a min threshold of 155 and an upper threshold of 200, did a better job than the S channel in identifying the yellow lines, but completely ignored the white lines.\n",
    "I chose to create a combined binary threshold based on the three above mentioned binary thresholds, to create one combination thresholded image which does a great job of highlighting almost all of the white and yellow lane lines.\n",
    "\n",
    "**Note**: The S binary threshold was left out of the final combined binary image and was not used in detecting lane lines because it added extra noise to the binary image and interfered with detecting lane lines accurately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create binary thresholded images to isolate lane line pixels\n",
    "# expects a undistorted view img\n",
    "def apply_color_thresholds(img, display=True):       \n",
    "\n",
    "    s_channel = cv2.cvtColor(img, cv2.COLOR_BGR2HLS)[:,:,2]\n",
    "    \n",
    "    l_channel = cv2.cvtColor(img, cv2.COLOR_BGR2LUV)[:,:,0]\n",
    "\n",
    "    b_channel = cv2.cvtColor(img, cv2.COLOR_BGR2Lab)[:,:,2]   \n",
    "\n",
    "    # Threshold color channel\n",
    "    s_thresh_min = 180\n",
    "    s_thresh_max = 255\n",
    "    s_binary = np.zeros_like(s_channel)\n",
    "    s_binary[(s_channel >= s_thresh_min) & (s_channel <= s_thresh_max)] = 1\n",
    "    \n",
    "    b_thresh_min = 155\n",
    "    b_thresh_max = 200\n",
    "    b_binary = np.zeros_like(b_channel)\n",
    "    b_binary[(b_channel >= b_thresh_min) & (b_channel <= b_thresh_max)] = 1\n",
    "    \n",
    "    l_thresh_min = 225\n",
    "    l_thresh_max = 255\n",
    "    l_binary = np.zeros_like(l_channel)\n",
    "    l_binary[(l_channel >= l_thresh_min) & (l_channel <= l_thresh_max)] = 1\n",
    "\n",
    "    #color_binary = np.dstack((u_binary, s_binary, l_binary))\n",
    "    \n",
    "    combined_binary = np.zeros_like(s_binary)\n",
    "    combined_binary[(l_binary == 1) | (b_binary == 1)] = 1\n",
    "\n",
    "    if display == True:\n",
    "        # Plotting thresholded images\n",
    "        f, ((ax1, ax2, ax3), (ax4,ax5, ax6)) = plt.subplots(2, 3, sharey='col', sharex='row', figsize=(10,4))\n",
    "        f.tight_layout()\n",
    "        \n",
    "        ax1.set_title('Original Image', fontsize=16)\n",
    "        ax1.imshow(cv2.cvtColor(undistort(calibration, image, display=False),cv2.COLOR_BGR2RGB))\n",
    "        \n",
    "        ax2.set_title('Warped Image', fontsize=16)\n",
    "        ax2.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB).astype('uint8'))\n",
    "        \n",
    "        ax3.set_title('s binary threshold', fontsize=16)\n",
    "        ax3.imshow(s_binary, cmap='gray')\n",
    "        \n",
    "        ax4.set_title('b binary threshold', fontsize=16)\n",
    "        ax4.imshow(b_binary, cmap='gray')\n",
    "        \n",
    "        ax5.set_title('l binary threshold', fontsize=16)\n",
    "        ax5.imshow(l_binary, cmap='gray')\n",
    "\n",
    "        ax6.set_title('Combined color thresholds', fontsize=16)\n",
    "        ax6.imshow(combined_binary, cmap='gray')\n",
    "        \n",
    "        \n",
    "    else: \n",
    "        return combined_binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the function\n",
    "# Comment out this block for final run\n",
    "for image in glob.glob('test_images/test*.jpg'):\n",
    "    \n",
    "# Undistort a test image\n",
    "    img = cv2.imread(image)\n",
    "    undist = undistort(calibration,image)\n",
    "    # Calculate directional gradient\n",
    "    grad_x = abs_sobel_thresh(undist, orient='x', sobel_kernel=15, thresh=(30, 100))\n",
    "\n",
    "    grad_y = abs_sobel_thresh(undist, orient='y', sobel_kernel=15, thresh=(30, 100))\n",
    "\n",
    "    # Calculate gradient magnitude \n",
    "    mag_binary = mag_thresh(undist, sobel_kernel=15, mag_thresh=(50, 100))\n",
    "\n",
    "    # Calculate gradient direction\n",
    "    dir_binary = dir_thresh(undist, sobel_kernel=15, dir_thresh=(0.7, 1.3))\n",
    "\n",
    "    # Calculate color threshold\n",
    "    col_binary = apply_color_thresholds(undist, display = False) \n",
    "    #combined = apply_color_thresholds(wrap, display = False) \n",
    "\n",
    "    # Combine all the thresholds to identify the lane lines\n",
    "    combined = combine_threshs(grad_x, grad_y, mag_binary, dir_binary,col_binary, ksize=15)   \n",
    "    warped, M , Minv = birds_eye(image,calibration,display = False)\n",
    "    apply_color_thresholds(warped,True)\n",
    "    combined = apply_color_thresholds(warped,False)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the function\n",
    "# Comment out this block for final run\n",
    "for image in glob.glob('test_images/test*.jpg'):\n",
    "    undist = undistort(calibration,image)\n",
    "    #warped, M , Minv = birds_eye(image,calibration,display = False)\n",
    "    apply_color_thresholds(warped,True)\n",
    "    combined = apply_color_thresholds(warped,False)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_threshs(binary_gradient_x, binary_gradient_y,binary_grad_mag, binary_grad_dir,binary_color,  ksize=15):\n",
    "    \n",
    "    # Combine the previous thresholds\n",
    "    combined = np.zeros_like(binary_grad_dir)\n",
    "    \n",
    "    combined[(((binary_gradient_x == 1) & (binary_gradient_y == 1)) |\n",
    "             ((binary_grad_mag == 1) & (binary_grad_dir == 1)))  |\n",
    "             (binary_color == 1)] = 1\n",
    "\n",
    "    return combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Fitting a polynomial to the lane lines\n",
    "\n",
    "Output visual display of the lane boundaries and numerical estimation of lane curvature and vehicle position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_histogram(img, display = True):\n",
    "    \n",
    "    histogram  = np.sum(img[img.shape[0]//2:, :], axis=0)\n",
    "    \n",
    "    if display :   \n",
    "\n",
    "        # Plot the results\n",
    "        plt.title('Histogram', fontsize=16)\n",
    "        plt.xlabel('Pixel position')\n",
    "        plt.ylabel('Counts')\n",
    "        plt.plot(histogram)\n",
    "        return histogram\n",
    "\n",
    "    else :\n",
    "        return histogram\n",
    "\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'combined' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-3072d669fd1e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Comment out this block for final run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Run de function over the combined warped image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mhistogram\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_histogram\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcombined\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'combined' is not defined"
     ]
    }
   ],
   "source": [
    "# Testing the function\n",
    "# Comment out this block for final run\n",
    "# Run de function over the combined warped image\n",
    "histogram = get_histogram(combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_lines(img, return_img=False):\n",
    "    # Take a histogram of the bottom half of the image\n",
    "    histogram = get_histogram(img,display = False)\n",
    "\n",
    "    if return_img:\n",
    "        # Create an output image to draw on and  visualize the result\n",
    "        out_img = np.dstack((img, img, img))*255\n",
    "    \n",
    "    # Find the peak of the left and right halves of the histogram\n",
    "    # These will be the starting point for the left and right lines\n",
    "    midpoint = np.int(histogram.shape[0]//2)\n",
    "    leftx_base = np.argmax(histogram[:midpoint])\n",
    "    rightx_base = np.argmax(histogram[midpoint:]) + midpoint\n",
    "\n",
    "    # Choose the number of sliding windows\n",
    "    nwindows = 20\n",
    "    \n",
    "    # Set height of windows\n",
    "    window_height = np.int(img.shape[0]//nwindows)\n",
    "    \n",
    "    # Identify the x and y positions of all nonzero pixels in the image\n",
    "    nonzero = img.nonzero()\n",
    "    nonzerox = np.array(nonzero[1])   \n",
    "    nonzeroy = np.array(nonzero[0])\n",
    "   \n",
    "\n",
    "    # Current positions to be updated for each window\n",
    "    leftx_current = leftx_base\n",
    "    rightx_current = rightx_base\n",
    "    \n",
    "    # Set the width of the windows +/- margin\n",
    "    margin = 100\n",
    "\n",
    "    # Set minimum number of pixels found to recenter window\n",
    "    minpix = 50\n",
    "    \n",
    "    # Create empty lists to receive left and right lane pixel indices\n",
    "    left_lane_inds = []\n",
    "    right_lane_inds = []\n",
    "\n",
    "    # Step through the windows one by one\n",
    "    for window in range(nwindows):\n",
    "        # Identify window boundaries in x and y (and right and left)\n",
    "        win_y_low = img.shape[0] - (window + 1)*window_height\n",
    "        win_y_high = img.shape[0] - window*window_height\n",
    "        win_xleft_low = leftx_current - margin\n",
    "        win_xleft_high = leftx_current + margin\n",
    "        win_xright_low = rightx_current - margin\n",
    "        win_xright_high = rightx_current + margin\n",
    "        \n",
    "        if return_img:\n",
    "            # Draw the windows on the visualization image\n",
    "            cv2.rectangle(out_img,(win_xleft_low,win_y_low),(win_xleft_high,win_y_high), (0,255,0), 3) \n",
    "            cv2.rectangle(out_img,(win_xright_low,win_y_low),(win_xright_high,win_y_high), (0,255,0), 3) \n",
    "\n",
    "        # Identify the nonzero pixels in x and y within the window\n",
    "        good_left_inds = ((nonzeroy >= win_y_low) & (nonzeroy < win_y_high) & \n",
    "                          (nonzerox >= win_xleft_low) &  (nonzerox < win_xleft_high)).nonzero()[0]\n",
    "        good_right_inds = ((nonzeroy >= win_y_low) & (nonzeroy < win_y_high) & \n",
    "                           (nonzerox >= win_xright_low) &  (nonzerox < win_xright_high)).nonzero()[0]\n",
    "        # Append these indices to the lists\n",
    "        left_lane_inds.append(good_left_inds)\n",
    "        right_lane_inds.append(good_right_inds)\n",
    "        \n",
    "        # If you found > minpix pixels, recenter next window on their mean position\n",
    "        if len(good_left_inds) > minpix:\n",
    "            leftx_current = np.int(np.mean(nonzerox[good_left_inds]))\n",
    "        if len(good_right_inds) > minpix:        \n",
    "            rightx_current = np.int(np.mean(nonzerox[good_right_inds]))\n",
    "\n",
    "    # Concatenate the arrays of indices\n",
    "    left_lane_inds = np.concatenate(left_lane_inds)\n",
    "    right_lane_inds = np.concatenate(right_lane_inds)\n",
    "    \n",
    "    # Extract left and right line pixel positions\n",
    "    leftx = nonzerox[left_lane_inds]\n",
    "    lefty = nonzeroy[left_lane_inds] \n",
    "    rightx = nonzerox[right_lane_inds]\n",
    "    righty = nonzeroy[right_lane_inds] \n",
    "\n",
    "    # Fit a second order polynomial to each\n",
    "    left_fit = np.polyfit(lefty, leftx, 2)\n",
    "    right_fit = np.polyfit(righty, rightx, 2)\n",
    "    \n",
    "    ploty = np.linspace(0, img.shape[0] - 1, img.shape[0])\n",
    "    left_fitx = left_fit[0]*ploty**2 + left_fit[1]*ploty + left_fit[2]\n",
    "    right_fitx = right_fit[0]*ploty**2 + right_fit[1]*ploty + right_fit[2]\n",
    "    \n",
    "    if return_img:\n",
    "        out_img[nonzeroy[left_lane_inds], nonzerox[left_lane_inds]] = [255, 0, 0]\n",
    "        out_img[nonzeroy[right_lane_inds], nonzerox[right_lane_inds]] = [0, 0, 255]\n",
    "\n",
    "        # Draw left and right lines\n",
    "        for index in range(img.shape[0]):\n",
    "            cv2.circle(out_img, (int(left_fitx[index]), int(ploty[index])), 3, (255,255,0))\n",
    "            cv2.circle(out_img, (int(right_fitx[index]), int(ploty[index])), 3, (255,255,0))\n",
    "            \n",
    "        return (left_fit, right_fit), (left_fitx, ploty), (right_fitx, ploty), out_img.astype(int)\n",
    "\n",
    "    return (left_fit, right_fit), (left_fitx, ploty), (right_fitx, ploty)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the function\n",
    "# Comment out this block for final run\n",
    "\n",
    "# Run the function\n",
    "lines_fit, left_points, right_points, out_img = detect_lines(combined, return_img=True)\n",
    "\n",
    "# Plot the results\n",
    "plt_images(combined, 'Warped image', out_img, 'Lane lines detected')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_similar_lines(img, line_fits=None, return_img=False):\n",
    "    if line_fits is None:\n",
    "        return detect_lines(img, return_img)\n",
    "    \n",
    "    left_fit = line_fits[0]\n",
    "    right_fit = line_fits[1]\n",
    "    \n",
    "    nonzero = img.nonzero()\n",
    "    nonzeroy = np.array(nonzero[0])\n",
    "    nonzerox = np.array(nonzero[1])\n",
    "    margin = 100\n",
    "    left_lane_inds = ((nonzerox > (left_fit[0]*(nonzeroy**2) + left_fit[1]*nonzeroy + \n",
    "    left_fit[2] - margin)) & (nonzerox < (left_fit[0]*(nonzeroy**2) + \n",
    "    left_fit[1]*nonzeroy + left_fit[2] + margin))) \n",
    "\n",
    "    right_lane_inds = ((nonzerox > (right_fit[0]*(nonzeroy**2) + right_fit[1]*nonzeroy + \n",
    "    right_fit[2] - margin)) & (nonzerox < (right_fit[0]*(nonzeroy**2) + \n",
    "    right_fit[1]*nonzeroy + right_fit[2] + margin)))  \n",
    "\n",
    "    # Again, extract left and right line pixel positions\n",
    "    leftx = nonzerox[left_lane_inds]\n",
    "    lefty = nonzeroy[left_lane_inds] \n",
    "    rightx = nonzerox[right_lane_inds]\n",
    "    righty = nonzeroy[right_lane_inds]\n",
    "\n",
    "    # If any of the lines could not be found, \n",
    "    # perform a more exhaustive search\n",
    "    if (leftx.size == 0 or rightx.size == 0):\n",
    "        return detect_lines(img, return_img)\n",
    "    \n",
    "    # Fit a second order polynomial to each\n",
    "    left_fit = np.polyfit(lefty, leftx, 2)\n",
    "    right_fit = np.polyfit(righty, rightx, 2)\n",
    "    \n",
    "    # Generate x and y values for plotting\n",
    "    ploty = np.linspace(0, img.shape[0] - 1, img.shape[0] )\n",
    "    left_fitx = left_fit[0]*ploty**2 + left_fit[1]*ploty + left_fit[2]\n",
    "    right_fitx = right_fit[0]*ploty**2 + right_fit[1]*ploty + right_fit[2]\n",
    "        \n",
    "    if return_img:\n",
    "        # Create an image to draw on and an image to show the selection window\n",
    "        out_img = np.dstack((img, img, img))*255\n",
    "        window_img = np.zeros_like(out_img)\n",
    "\n",
    "        # Generate a polygon to illustrate the search window area\n",
    "        # And recast the x and y points into usable format for cv2.fillPoly()\n",
    "        left_line_window1 = np.array([np.transpose(np.vstack([left_fitx - margin, ploty]))])\n",
    "        left_line_window2 = np.array([np.flipud(np.transpose(np.vstack([left_fitx + margin, ploty])))])\n",
    "        left_line_pts = np.hstack((left_line_window1, left_line_window2))\n",
    "        right_line_window1 = np.array([np.transpose(np.vstack([right_fitx - margin, ploty]))])\n",
    "        right_line_window2 = np.array([np.flipud(np.transpose(np.vstack([right_fitx + margin, ploty])))])\n",
    "        right_line_pts = np.hstack((right_line_window1, right_line_window2))\n",
    "\n",
    "        # Draw the lane onto the warped blank image\n",
    "        cv2.fillPoly(window_img, np.int_([left_line_pts]), (0,255, 0))\n",
    "        cv2.fillPoly(window_img, np.int_([right_line_pts]), (0,255, 0))\n",
    "        out_img = cv2.addWeighted(out_img, 1, window_img, 0.3, 0)\n",
    "\n",
    "        # Color in left and right line pixels\n",
    "        out_img[nonzeroy[left_lane_inds], nonzerox[left_lane_inds]] = [255, 0, 0]\n",
    "        out_img[nonzeroy[right_lane_inds], nonzerox[right_lane_inds]] = [0, 0, 255]\n",
    "     \n",
    "        for index in range(img.shape[0]):\n",
    "            cv2.circle(out_img, (int(left_fitx[index]), int(ploty[index])), 3, (255,255,0))\n",
    "            cv2.circle(out_img, (int(right_fitx[index]), int(ploty[index])), 3, (255,255,0))\n",
    "    \n",
    "        return (left_fit, right_fit), (left_fitx, ploty), (right_fitx, ploty), out_img.astype(int)\n",
    "    \n",
    "    return (left_fit, right_fit), (left_fitx, ploty), (right_fitx, ploty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the function\n",
    "# Notice I am passing the same image than before.\n",
    "# In a video stream, it should be passed the next frame.\n",
    "lines_fit, left_points, right_points, out_img = detect_similar_lines(combined, lines_fit, return_img=True)\n",
    "\n",
    "# Plot the results\n",
    "plt_images(combined, 'Warped image', out_img, 'Lane lines detected')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6 : Radis of Curvature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def curvature_radius (leftx, rightx, img_shape, xm_per_pix=3.7/800, ym_per_pix = 25/720):\n",
    "    ploty = np.linspace(0, img_shape[0] - 1, img_shape[0])\n",
    "    \n",
    "    leftx = leftx[::-1]  # Reverse to match top-to-bottom in y\n",
    "    rightx = rightx[::-1]  # Reverse to match top-to-bottom in y\n",
    "    \n",
    "    # Fit a second order polynomial to pixel positions in each fake lane line\n",
    "    left_fit = np.polyfit(ploty, leftx, 2)\n",
    "    left_fitx = left_fit[0]*ploty**2 + left_fit[1]*ploty + left_fit[2]\n",
    "    right_fit = np.polyfit(ploty, rightx, 2)\n",
    "    right_fitx = right_fit[0]*ploty**2 + right_fit[1]*ploty + right_fit[2]\n",
    "\n",
    "    # Define conversions in x and y from pixels space to meters\n",
    "    ym_per_pix = 25/720 # meters per pixel in y dimension\n",
    "    xm_per_pix = 3.7/800 # meters per pixel in x dimension\n",
    "\n",
    "    # Fit new polynomials to x,y in world space\n",
    "    y_eval = np.max(ploty)\n",
    "    left_fit_cr = np.polyfit(ploty*ym_per_pix, leftx*xm_per_pix, 2)\n",
    "    right_fit_cr = np.polyfit(ploty*ym_per_pix, rightx*xm_per_pix, 2)\n",
    "    \n",
    "    # Calculate the new radii of curvature\n",
    "    left_curverad = ((1 + (2*left_fit_cr[0]*y_eval*ym_per_pix + left_fit_cr[1])**2)**1.5) / np.absolute(2*left_fit_cr[0])\n",
    "    right_curverad = ((1 + (2*right_fit_cr[0]*y_eval*ym_per_pix + right_fit_cr[1])**2)**1.5) / np.absolute(2*right_fit_cr[0])\n",
    "    \n",
    "    # Now our radius of curvature is in meters\n",
    "    return (left_curverad, right_curverad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the function\n",
    "# Comment out this block for final run\n",
    "# Run the function\n",
    "curvature_rads = curvature_radius(leftx=left_points[0], rightx=right_points[0], img_shape = combined.shape)\n",
    "\n",
    "# Print the results\n",
    "print('Left line curvature:', curvature_rads[0], 'm')\n",
    "print('Right line curvature:', curvature_rads[1], 'm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7 : Calculating Vehicle Position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def car_offset(leftx, rightx, img_shape, xm_per_pix=3.7/800):\n",
    "    ## Image mid horizontal position \n",
    "    mid_imgx = img_shape[1]//2\n",
    "        \n",
    "    ## Car position with respect to the lane\n",
    "    car_pos = (leftx[-1] + rightx[-1])/2\n",
    "    \n",
    "    ## Horizontal car offset \n",
    "    offsetx = (mid_imgx - car_pos) * xm_per_pix\n",
    "\n",
    "    return offsetx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the function\n",
    "# Comment out this block for final run\n",
    "# Run the function\n",
    "offsetx = car_offset(leftx=left_points[0], rightx=right_points[0], img_shape=img.shape)\n",
    "\n",
    "print ('Car offset from center:', offsetx, 'm.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Warp the detected lane boundaries back onto the original image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_lane(img, warped_img, left_points, right_points, Minv):\n",
    "    # Create an image to draw the lines on\n",
    "    warp_zero = np.zeros_like(warped_img).astype(np.uint8)\n",
    "    color_warp = np.dstack((warp_zero, warp_zero, warp_zero))\n",
    "\n",
    "    # Recast the x and y points into usable format for cv2.fillPoly()\n",
    "    left_fitx = left_points[0]\n",
    "    right_fitx = right_points[0]\n",
    "    ploty = left_points[1]\n",
    "\n",
    "    pts_left = np.array([np.transpose(np.vstack([left_fitx, ploty]))])\n",
    "    pts_right = np.array([np.flipud(np.transpose(np.vstack([right_fitx, ploty])))])\n",
    "    pts = np.hstack((pts_left, pts_right))\n",
    "\n",
    "    # Draw the lane onto the warped blank image\n",
    "    cv2.fillPoly(color_warp, np.int_([pts]), (0,255, 0))\n",
    "\n",
    "    # Warp the blank back to original image space using inverse perspective matrix (Minv)\n",
    "    newwarp = cv2.warpPerspective(color_warp, Minv, (img.shape[1], img.shape[0])) \n",
    "\n",
    "    # Combine the result with the original image\n",
    "    return cv2.addWeighted(img, 1, newwarp, 0.3, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the function\n",
    "# Comment out this block for final run\n",
    "\n",
    "# Run the function\n",
    "img_lane = draw_lane(img, combined, left_points, right_points, Minv)\n",
    "\n",
    "# Plot the results\n",
    "plt_images(img, 'Test image', img_lane, 'Lane detected')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def add_metrics(img, leftx, rightx, xm_per_pix=3.7/800, ym_per_pix = 25/720):    \n",
    "    # Calculate radius of curvature\n",
    "    curvature_rads = curvature_radius(leftx=leftx, rightx=rightx, img_shape=img.shape,\n",
    "                                      xm_per_pix=xm_per_pix, ym_per_pix=ym_per_pix)\n",
    "    # Calculate car offset\n",
    "    offsetx = car_offset(leftx=leftx, rightx=rightx, img_shape=img.shape)\n",
    "\n",
    "    # Display lane curvature\n",
    "    out_img = img.copy()\n",
    "    cv2.putText(out_img, 'Left lane line curvature: {:.2f} m'.format(curvature_rads[0]), \n",
    "                (60, 60), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (255,255,255), 5)\n",
    "    cv2.putText(out_img, 'Right lane line curvature: {:.2f} m'.format(curvature_rads[1]), \n",
    "                (60, 110), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (255,255,255), 5)\n",
    "    \n",
    "    # Display car offset\n",
    "    cv2.putText(out_img, 'Horizontal car offset: {:.2f} m'.format(offsetx), \n",
    "                (60, 160), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (255,255,255), 5)\n",
    "    \n",
    "    return out_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the function\n",
    "# Comment out this block for final run\n",
    "# Run the function\n",
    "out_img = add_metrics(img_lane, leftx=left_points[0], rightx=right_points[0])\n",
    "\n",
    "# Plot the results\n",
    "plt_images(img, 'Test image', out_img, 'Lane detected with metrics')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProcessImage:\n",
    "    def __init__(self, image_path):\n",
    "        # Make a list of calibration images\n",
    "        #images = glob.glob(images)\n",
    "\n",
    "        # Calibrate camera\n",
    "        self.calibration = calibrate_camera(image_path)\n",
    "        self.lines_fit = None\n",
    "\n",
    "    def __call__(self, img):\n",
    "        # Undistord image\n",
    "        img = undistort(self.calibration, img, display = False , read = False)         \n",
    "      \n",
    "        # Calculate directional gradient\n",
    "        grad_x = abs_sobel_thresh(img, orient='x', sobel_kernel=15, thresh=(30, 100))\n",
    "        \n",
    "        grad_y = abs_sobel_thresh(img, orient='y', sobel_kernel=15, thresh=(30, 100))\n",
    "\n",
    "        # Calculate gradient magnitude \n",
    "        mag_binary = mag_thresh(img, sobel_kernel=15, mag_thresh=(50, 100))\n",
    "\n",
    "        # Calculate gradient direction\n",
    "        dir_binary = dir_thresh(img, sobel_kernel=15, dir_thresh=(0.7, 1.3))\n",
    "        \n",
    "        # Calculate color threshold\n",
    "        col_binary = apply_color_thresholds(img, display = False) \n",
    "        #combined = apply_color_thresholds(wrap, display = False) \n",
    "        \n",
    "        # Combine all the thresholds to identify the lane lines\n",
    "        combined = combine_threshs(grad_x, grad_y, mag_binary, dir_binary,col_binary, ksize=15) \n",
    "        \n",
    "        # Apply a perspective transform to rectify binary image (\"birds-eye view\")\n",
    "        # Since we are passing un-distorted img, calibration note required again\n",
    "        combined_wrap, M , Minv = birds_eye(combined,display = False)   \n",
    "        self.lines_fit, left_points, right_points, out_img = detect_similar_lines(combined_wrap, self.lines_fit, return_img=True)\n",
    "        \n",
    "        # Warp the detected lane boundaries back onto the original image.\n",
    "        img_lane = draw_lane(img, combined, left_points, right_points, Minv)\n",
    "\n",
    "            \n",
    "        # Add metrics to the output img\n",
    "        out_img = add_metrics(img_lane, leftx=left_points[0], rightx=right_points[0])\n",
    "            \n",
    "        return out_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing the function\n",
    "# Comment out this block for final run\n",
    "\n",
    "# Process video frames with our 'process_image' function\n",
    "process_image = ProcessImage('./camera_cal/calibration*.jpg')\n",
    "\n",
    "# Process video frames with our 'process_image' function\n",
    "img = cv2.imread('test_images/test2.jpg')\n",
    "process_image(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Camera successfully calibrated.\n",
      "[MoviePy] >>>> Building video ./project_video_solution.mp4\n",
      "[MoviePy] Writing video ./project_video_solution.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 1260/1261 [05:45<00:00,  3.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] Done.\n",
      "[MoviePy] >>>> Video ready: ./project_video_solution.mp4 \n",
      "\n",
      "CPU times: user 16min 30s, sys: 1min 4s, total: 17min 35s\n",
      "Wall time: 5min 45s\n"
     ]
    }
   ],
   "source": [
    "input_video = './project_video.mp4'\n",
    "output_video = './project_video_solution.mp4'\n",
    "\n",
    "\n",
    "## You may uncomment the following line for a subclip of the first 5 seconds\n",
    "#clip1 = VideoFileClip(input_video).subclip(0,5)\n",
    "clip1 = VideoFileClip(input_video)\n",
    "\n",
    "# Process video frames with our 'process_image' function\n",
    "process_image = ProcessImage('./camera_cal/calibration*.jpg')\n",
    "\n",
    "white_clip = clip1.fl_image(process_image)\n",
    "\n",
    "%time white_clip.write_videofile(output_video, audio=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"640\" height=\"360\" controls>\n",
       "  <source src=\"./project_video_solution.mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"360\" controls>\n",
    "  <source src=\"{0}\">\n",
    "</video>\n",
    "\"\"\".format(output_video))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_video = './challenge_video.mp4'\n",
    "output_video = './challenge_video_solution.mp4'\n",
    "\n",
    "# Process video frames with our 'process_image' function\n",
    "process_image = ProcessImage('./camera_cal/calibration*.jpg')\n",
    "\n",
    "## You may uncomment the following line for a subclip of the first 5 seconds\n",
    "#clip1 = VideoFileClip(input_video).subclip(0,5)\n",
    "clip1 = VideoFileClip(input_video)\n",
    "white_clip = clip1.fl_image(process_image)\n",
    "\n",
    "%time white_clip.write_videofile(output_video, audio=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"640\" height=\"360\" controls>\n",
       "  <source src=\"./challenge_video_solution.mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"360\" controls>\n",
    "  <source src=\"{0}\">\n",
    "</video>\n",
    "\"\"\".format(output_video))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Harder Challenge Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] >>>> Building video ./harder_challenge_video_solution.mp4\n",
      "[MoviePy] Writing video ./harder_challenge_video_solution.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 125/126 [00:34<00:00,  3.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] Done.\n",
      "[MoviePy] >>>> Video ready: ./harder_challenge_video_solution.mp4 \n",
      "\n",
      "CPU times: user 1min 39s, sys: 6.81 s, total: 1min 46s\n",
      "Wall time: 35 s\n"
     ]
    }
   ],
   "source": [
    "input_video = './harder_challenge_video.mp4'\n",
    "output_video = './harder_challenge_video_solution.mp4'\n",
    "\n",
    "## You may uncomment the following line for a subclip of the first 5 seconds\n",
    "#clip1 = VideoFileClip(input_video).subclip(0,5)\n",
    "clip1 = VideoFileClip(input_video)\n",
    "white_clip = clip1.fl_image(process_image)\n",
    "\n",
    "%time white_clip.write_videofile(output_video, audio=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"640\" height=\"360\" controls>\n",
       "  <source src=\"./harder_challenge_video_solution.mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"360\" controls>\n",
    "  <source src=\"{0}\">\n",
    "</video>\n",
    "\"\"\".format(output_video))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
